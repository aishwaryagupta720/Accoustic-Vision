# MiniProject
## THIS PROJECT WILL HAVE THE FOLLOWING SCOPE:
-AN ASSET AS WELL AS COMPANION FOR VISUALLY IMPAIRED PEOPLE
-EASING OUT THE WORK OF BLIND BY  DETECTING OBJECTS AND PEOPLEâ€™S FACE.
-THE APP CAN BE FURTHER EXTENDED WITH VARIOUS FUNCTIONALITIES LIKE NAVIGATION ,VIDEO CALLING AND SOS FUNCTIONALITY 

## Software Requirements:

-Google FireBase

-Tensorflow Library

-GoogleMaps API key

-Speech To Text APIs

-Text to Speech APIs

## Flow of Code:
### Walkthrough:
It contains Slide screens that explain the utility of the code in 4 slides

### Login:
User Interface to provide Registration form for first time user.
This form can be filled using only voice input using the Speech to Text API from Google ML kit
One time Registration and login for security purposes

### MainMenu:
Menu is read out using Text to speech API: Firebase ML KIT
Press the __Speak__ button and Using the Keyword __Vision__ we can name the button to be redirected accordingly

Geographically Placed buttons on the Screen with distict functions:

__SOS__: On single click or Voice Command sends SOS message to 5 close relatives with Geo-Location using Google maps API and a call is dialed to the closest relative

__Text Reading__: On clicking this button Text Detection API :Tensorflow comes into use which can read any font or style of text from almost any surface with 95% accuracy. This text is then Read out using Text to Speech API 

__Detection__: This Button redirects us to the Object Detection API:Tensorflow which detects objects on screen with similar(90%) accuracy.
Text-to-speech API is again used to read it out to the user.
This UI can also Detect Expressions(based on smile curve) and Face Features of the person In front of the camera using Face Detection Algorithm:Opensource.

__Profile__: For the user profile filled at the time of registration and future edits and changes. 

__Settings__: Text to speech can be disabled here





